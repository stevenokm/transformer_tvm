{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from instruct_pipeline import InstructionTextGenerationPipeline\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args.input_model = \"dolly-v2-3b\"\n",
    "args.gpu_family = \"a2000\"\n",
    "args.timestamp = int(time.time())\n",
    "\n",
    "logFormatter = logging.Formatter(\n",
    "    \"%(asctime)s \" + \"[%(threadName)-12.12s] \" + \"[%(levelname)-5.5s]  \" + \"%(message)s\"\n",
    ")\n",
    "\n",
    "fileHandler = logging.FileHandler(\"dolly-v2-3b_{}.log\".format(int(args.timestamp)))\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "fileHandler.setLevel(logging.DEBUG)\n",
    "\n",
    "consoleHandler = logging.StreamHandler(sys.stdout)\n",
    "consoleHandler.setLevel(logging.INFO)\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "\n",
    "logging.basicConfig(handlers=[fileHandler, consoleHandler], level=logging.DEBUG)\n",
    "\n",
    "logging.debug(str(sys.path))\n",
    "logging.debug(str(os.environ[\"PYTHONPATH\"]))\n",
    "\n",
    "start_time = int(args.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup random seed\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"setup model\")\n",
    "\n",
    "if (\n",
    "    args.input_model == \"dolly-v2-3b\"\n",
    "    or args.input_model == \"dolly-v2-7b\"\n",
    "    or args.input_model == \"dolly-v2-13b\"\n",
    "):\n",
    "    model_path_or_name = \"databricks/{}\".format(args.input_model)\n",
    "else:\n",
    "    model_path_or_name = os.path.expanduser(os.path.expandvars(args.input_model))\n",
    "\n",
    "# configure the batch_size\n",
    "batch_size = 4\n",
    "if args.gpu_family == \"a10\":\n",
    "    batch_size = 6\n",
    "elif args.gpu_family == \"a100\":\n",
    "    batch_size = 8\n",
    "elif args.gpu_family == \"a2000\":\n",
    "    batch_size = 4\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "\n",
    "device_map = None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.float32,\n",
    "    torchscript=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "if hasattr(model, \"hf_device_map\"):\n",
    "    logging.info(\"device_map: {}\".format(model.hf_device_map))\n",
    "\n",
    "generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add autoTVM on GPU\n",
    "# ref: https://tvm.apache.org/docs/how_to/tune_with_autoscheduler/tune_network_cuda.html\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tvm\n",
    "from tvm import relay, auto_scheduler, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network and compilation target\n",
    "network = \"dolly-v2-3b\"\n",
    "batch_size = 2\n",
    "sequence_size = 128\n",
    "layout = \"NL\"\n",
    "target = tvm.target.Target(\"cuda\")\n",
    "dtype = \"float32\"\n",
    "log_file = \"%s-%s-B%d-%s.json\" % (network, layout, batch_size, target.kind.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare network\n",
    "query = \"Explain to me the difference between nuclear fission and fusion.\"\n",
    "\n",
    "# generate sequence ids for model input\n",
    "model_inputs = generate_text.preprocess(query)\n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "if batch_size > 1:\n",
    "    for input_name in input_names:\n",
    "        model_inputs[input_name] = model_inputs[input_name].repeat(batch_size, 1)\n",
    "dummy_inputs = [model_inputs[input_name] for input_name in input_names]\n",
    "batch_size = model_inputs[\"input_ids\"].shape[0]\n",
    "sequence_size = model_inputs[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace model\n",
    "for para in model.parameters():\n",
    "    para.requires_grad = False\n",
    "\n",
    "traced_file = \"{}_traced.pt\".format(network)\n",
    "if os.path.exists(traced_file):\n",
    "    logging.info(\"Load traced model...\")\n",
    "    scripted_model = torch.jit.load(traced_file)\n",
    "else:\n",
    "    logging.info(\"Trace model...\")\n",
    "    scripted_model = torch.jit.trace(\n",
    "        model,\n",
    "        dummy_inputs,\n",
    "    )\n",
    "    torch.jit.save(scripted_model, traced_file)\n",
    "scripted_model.eval()\n",
    "for para in scripted_model.parameters():\n",
    "    para.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tasks from the network\n",
    "logging.info(\"Extract tasks...\")\n",
    "\n",
    "shape_list = [(input_name, [batch_size, sequence_size]) for input_name in input_names]\n",
    "mod, params = relay.frontend.from_pytorch(\n",
    "    scripted_model, shape_list, default_dtype=dtype\n",
    ")\n",
    "\n",
    "tasks, task_weights = auto_scheduler.extract_tasks(mod[\"main\"], params, target)\n",
    "\n",
    "for idx, task in enumerate(tasks):\n",
    "    logging.info(\n",
    "        \"========== Task %d  (workload key: %s) ==========\" % (idx, task.workload_key)\n",
    "    )\n",
    "    logging.debug(str(task.compute_dag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tuning():\n",
    "    logging.info(\"Begin tuning...\")\n",
    "    measure_ctx = auto_scheduler.LocalRPCMeasureContext(\n",
    "        repeat=1, min_repeat_ms=300, timeout=10\n",
    "    )\n",
    "\n",
    "    tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
    "    tune_option = auto_scheduler.TuningOptions(\n",
    "        num_measure_trials=200,  # change this to 20000 to achieve the best performance\n",
    "        runner=measure_ctx.runner,\n",
    "        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "    )\n",
    "\n",
    "    tuner.tune(tune_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with the history best\n",
    "# NOTE: use vm because graph executor does not support dynamic shape\n",
    "logging.info(\"Compile...\")\n",
    "with auto_scheduler.ApplyHistoryBest(log_file):\n",
    "    with tvm.transform.PassContext(\n",
    "        opt_level=3, config={\"relay.backend.use_auto_scheduler\": True}\n",
    "    ):\n",
    "        vmc = relay.vm.compile(mod, target=target, params=params)\n",
    "\n",
    "# Create VM runtime\n",
    "logging.info(\"Create runtime...\")\n",
    "dev = tvm.device(str(target), 0)\n",
    "module = runtime.vm.VirtualMachine(vmc, dev)\n",
    "for input_name in input_names:\n",
    "    module.set_input(\n",
    "        input_name, tvm.nd.array(model_inputs[input_name].numpy().astype(dtype))\n",
    "    )\n",
    "\n",
    "# Evaluate\n",
    "logging.info(\"Evaluate inference time cost...\")\n",
    "logging.info(str(module.benchmark(dev, repeat=3, min_repeat_ms=500)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
